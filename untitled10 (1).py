# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/117rSU8Oxj9TpofQMplzfVPuD5RR9zmpr
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve, f1_score, precision_score, recall_score
import pandas as pd
import numpy as np

def plot_confusion_matrix(y_true, y_pred, model_name="Model"):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap='Blues')
    plt.title(f"Confusion Matrix: {model_name}")
    plt.show()

def plot_roc_curve(y_true, y_probs, model_name="Model"):
    fpr, tpr, _ = roc_curve(y_true, y_probs)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')
    plt.plot([0,1], [0,1], linestyle='--', color='gray')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve: {model_name}')
    plt.legend(loc='lower right')
    plt.show()

def plot_precision_recall(y_true, y_probs, model_name="Model"):
    precision, recall, _ = precision_recall_curve(y_true, y_probs)
    plt.figure()
    plt.plot(recall, precision, label=model_name)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve: {model_name}')
    plt.legend(loc='upper right')
    plt.show()

def plot_feature_importance(model, feature_names, top_n=10):
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1][:top_n]
    plt.figure(figsize=(10,6))
    sns.barplot(x=importances[indices], y=np.array(feature_names)[indices])
    plt.title("Top Feature Importances")
    plt.xlabel("Importance Score")
    plt.show()

def compare_models(metrics_dict):
    df = pd.DataFrame(metrics_dict).T
    df.plot(kind='bar', figsize=(10,6))
    plt.title("Model Comparison (F1, Precision, Recall)")
    plt.ylabel("Score")
    plt.xticks(rotation=0)
    plt.ylim(0,1)
    plt.show()

# ===============================
# Example usage:
# Assume you have y_test, predictions, probabilities, and a trained model

# Example for one model (Random Forest)
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Sample data (replace with your real dataset)
X = pd.DataFrame({
    'transaction_amount': np.random.rand(1000),
    'hour_of_day': np.random.randint(0, 24, 1000),
    'location_mismatch': np.random.randint(0, 2, 1000)
})
y = np.random.randint(0, 2, 1000)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = RandomForestClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_probs = model.predict_proba(X_test)[:,1]

# Generate plots
plot_confusion_matrix(y_test, y_pred, "Random Forest")
plot_roc_curve(y_test, y_probs, "Random Forest")
plot_precision_recall(y_test, y_probs, "Random Forest")
plot_feature_importance(model, X.columns)

# Compare with other models
metrics = {
    "Random Forest": {
        "F1-score": f1_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred)
    },
    # Add other models here like:
    # "Logistic Regression": {...}
}
compare_models(metrics)

